{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd354156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(A, c, x):\n",
    "    tau = 1.0  # Parameter tau\n",
    "    lam = 1.0 # Parameter lambda\n",
    "    d = x.shape[0]  # Dimension of x\n",
    "\n",
    "    f0 = -tau * np.dot(c.T, x) + tau * lam * np.dot(x.T, np.dot(A.T, np.dot(A, x)))\n",
    "    \n",
    "    # ei âˆˆ Rd is the ith element of the canonical basis and 1 is row vector of all-ones\n",
    "    # Canonical basis elements\n",
    "    canonical_basis = np.eye(d)  \n",
    "    # Row vector of all ones\n",
    "    ones = np.ones(d)\n",
    "    \n",
    "    g = -np.sum(np.log(np.maximum(1e-15, np.dot(canonical_basis, x)))) - np.log(1 - np.dot(ones, x))\n",
    "\n",
    "    return f0 + g\n",
    "\n",
    "\n",
    "def gradient_f0(A, c, x, tau, lam):\n",
    "    return -tau * c + 2 * tau * lam * A.T @ A @ x\n",
    "\n",
    "def gradient_g(x):\n",
    "    d = x.shape[0]\n",
    "    canonical_basis = np.eye(d)  \n",
    "    ones = np.ones(d)\n",
    "    return - np.dot(canonical_basis.T, 1 / np.maximum(1e-15, np.dot(canonical_basis, x))) - ones / (1 - np.dot(ones, x))\n",
    "\n",
    "def gradient_f(A, c, x):\n",
    "    tau = 1.0  # Parameter tau\n",
    "    lam = 1.0 # Parameter lambda\n",
    "    grad_f0 = gradient_f0(A, c, x, tau, lam)\n",
    "    grad_g = gradient_g(x)\n",
    "    return grad_f0 + grad_g\n",
    "\n",
    "def hessian_f0(A):\n",
    "    tau = 1.0  # Parameter tau\n",
    "    lam = 1.0 # Parameter lambda\n",
    "    return tau * lam * np.dot(A.T, A)\n",
    "\n",
    "def hessian_g(x):\n",
    "    d = len(x)\n",
    "    diag_inv = np.diag(1 / (x ** 2))\n",
    "    ones = np.ones(d)\n",
    "    return diag_inv + np.outer(ones, ones)\n",
    "\n",
    "def hessian_f(A, x):\n",
    "    tau = 1.0  # Parameter tau\n",
    "    lam = 1.0 # Parameter lambda\n",
    "    return hessian_f0(A) + hessian_g(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7435b4ed",
   "metadata": {},
   "source": [
    "#### Original Newton's Method for Quadratic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion for original newton's method to solve QP problem\n",
    "def original_newton(A, c, x0, alpha, beta, max_iter=100000, eps=1e-2):\n",
    "    '''\n",
    "    This function implements the original Newton's update method on quadratic programming.\n",
    "    A: constraint matrix for quadratic programming.\n",
    "    c: linear term in the objective function.\n",
    "    alpha: the step size reduction factor for line search, taking values between 0 and 1.\n",
    "    beta: the sufficient decrease factor for line search.\n",
    "    max_iter: maximum number of iterations.\n",
    "    eps: tolerance for convergence.\n",
    "    '''\n",
    "    # Initialize variables\n",
    "    n, d = A.shape\n",
    "    xt = x0\n",
    "    \n",
    "    iteration = 0\n",
    "    opt_gaps = []\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        # Compute gradient and Hessian\n",
    "        grad = gradient_f(A, c, xt)\n",
    "        hessian = hessian_f(A, xt)\n",
    "\n",
    "        # Compute the Newton sketch update direction\n",
    "        direction = - np.dot(np.linalg.inv(hessian), grad)\n",
    "\n",
    "        # Compute the Newton sketch decrement\n",
    "        decrement = grad.T @ direction\n",
    "        \n",
    "        # Check convergence\n",
    "        if (decrement**2)/2 <= eps:\n",
    "            print(f\"Converged in {iteration+1} iterations.\")\n",
    "            break\n",
    "            \n",
    "\n",
    "        # Compute optimality gap\n",
    "        gap = np.linalg.norm(np.linalg.pinv(hessian) @ grad)\n",
    "        opt_gaps.append(gap)\n",
    "\n",
    "        # Line search to find the optimal step size\n",
    "        # Initial step size\n",
    "        mu = 0.05  \n",
    "        aux_xt = xt + mu * direction\n",
    "        \n",
    "        while quadratic(A, c, aux_xt) > quadratic(A, c, xt) +  alpha * mu * gap:\n",
    "            mu = beta*mu \n",
    "            aux_xt = xt + mu * direction         \n",
    "        \n",
    "        # Update solution\n",
    "        xt += mu * direction\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "                raise ValueError(\"Too many iterations\")\n",
    "    return opt_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quadratic programming problem\n",
    "n = 5 # Number of constraints\n",
    "d = 4 # Dimension of variables\n",
    "A = np.random.randn(n, d)  # Constraint matrix\n",
    "c = np.random.randn(d)  # Linear term\n",
    "x0 = np.random.randn(d)  # Initial iterate\n",
    "alpha = 0.5  # Step size reduction factor for line search\n",
    "beta = 0.8  # Sufficient decrease factor for line search\n",
    "\n",
    "# Solve the quadratic programming problem with Newton sketch method\n",
    "opt_gaps_original = original_newton(A, c, x0, alpha, beta)\n",
    "\n",
    "# Print the optimality gaps for each iteration\n",
    "print(\"Optimality gaps for sketching Newton's Method with fixed sketch dimension:\")\n",
    "print(opt_gaps_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot opt_gaps by iteration\n",
    "plt.plot(range(len(opt_gaps_original)), opt_gaps_original)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Optimality Gap')\n",
    "plt.title(\"Optimality Gap by Iteration: Original Newton's Method\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot opt_gaps by iteration\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(opt_gaps_original)), opt_gaps_original, label = 'Original Newton(n=5)')\n",
    "plt.plot(range(len(opt_gaps_newton5)), opt_gaps_newton5, label = 'Sketch Newton(n=5)')\n",
    "#plt.plot(range(len(opt_gaps_newton_et)), opt_gaps_newton_et, label = 'Sketch Newton(e(t))')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log Optimality Gap')\n",
    "plt.title(\"Log Optimality Gap by Iteration: Original Newton vs Sketch Newton\")\n",
    "# Set the y-axis scale to logarithmic\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032fa94",
   "metadata": {},
   "source": [
    "#### Sketching Newton's Method: fixed sketching accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def fwht(a) -> None:\n",
    "    h = 1\n",
    "    while h < len(a):\n",
    "        for i in range(0, len(a), h * 2):\n",
    "            for j in range(i, i + h):\n",
    "                a[j], a[j+h] = a[j]+a[j+h], a[j]-a[j+h]\n",
    "        h *= 2\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4261ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sketches based on randomized orthonormal systems (ROS)\n",
    "def sketch_ros(X, m):\n",
    "    \n",
    "    # Set up \n",
    "    n, d = X.shape\n",
    "    scalar = math.sqrt(n)\n",
    "    index = np.random.randint(0, d, size = m) # index from 0 to n-1 (means 1-n)\n",
    "\n",
    "\n",
    "    # Get PhiX, Phiy\n",
    "    D = np.random.choice([-1, 1], size = n)\n",
    "    \n",
    "    PX = np.zeros((m, d))\n",
    "\n",
    "    for i in range(d):\n",
    "        D = np.random.choice([-1, 1], size = n)\n",
    "        DX_i = D*X[:,i]\n",
    "        HDX_i = fwht(DX_i)\n",
    "        PX_i = scalar*np.random.choice(HDX_i, size = m)\n",
    "        PX[:,i] = PX_i\n",
    "\n",
    "    return(PX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_sketch(A, c, m, x0, alpha, beta, max_iter=1000000, eps=1e-2):\n",
    "    '''\n",
    "    This function implements the Newton sketch method on quadratic programming.\n",
    "    A: constraint matrix for quadratic programming.\n",
    "    c: linear term in the objective function.\n",
    "    m: sketch dimension.\n",
    "    alpha: the step size reduction factor for line search, taking values between 0 and 1.\n",
    "    beta: the sufficient decrease factor for line search.\n",
    "    max_iter: maximum number of iterations.\n",
    "    eps: tolerance for convergence.\n",
    "    '''\n",
    "    # Initialize variables\n",
    "    n, d = A.shape\n",
    "    xt = x0\n",
    "    \n",
    "    iteration = 0\n",
    "    opt_gaps = []\n",
    "    \n",
    "    while iteration < max_iter:\n",
    "        # Compute gradient and Hessian\n",
    "        # Compute gradient and Hessian\n",
    "        grad_f = gradient_f(A, c, xt)\n",
    "        h_f = hessian_f(A, xt)\n",
    "        \n",
    "        # Sketch the gradient and Hessian\n",
    "        sketch_hessian = sketch_ros(sqrtm(h_f), m)  # Generate sketched hessian matrix\n",
    "\n",
    "        \n",
    "        # Compute the approximate Newton step\n",
    "        delta_xt = - np.linalg.pinv(sketch_hessian.T @ sketch_hessian) @ grad_f\n",
    "        # Compute the approximate Newton decrement\n",
    "        lambda_xt = grad_f.T @ delta_xt\n",
    "        \n",
    "        # Check convergence\n",
    "        if (lambda_xt**2)/2 <= eps:\n",
    "            print(f\"Converged in {iteration+1} iterations.\")\n",
    "            break\n",
    "\n",
    "        # Compute optimality gap\n",
    "        gap = np.linalg.norm(np.linalg.pinv(h_f) @ grad_f)\n",
    "        opt_gaps.append(gap)\n",
    "\n",
    "        # Line search to find the optimal step size\n",
    "        # Initial step size\n",
    "        mu = 0.05\n",
    "        aux_xt = xt + mu * delta_xt\n",
    "        \n",
    "        while quadratic(A, c, aux_xt) > quadratic(A, c, xt) + alpha * mu * gap:\n",
    "            mu = beta * mu \n",
    "            aux_xt = xt + mu * delta_xt\n",
    "        \n",
    "        # Update solution\n",
    "        xt += mu * delta_xt\n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration >= max_iter:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    \n",
    "    return opt_gaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quadratic programming problem\n",
    "n = 5  # Number of constraints\n",
    "d = 4 # Dimension of variables\n",
    "A = np.random.randn(n, d)  # Constraint matrix\n",
    "c = np.random.randn(d)  # Linear term\n",
    "m = 4*d  # Sketch dimension\n",
    "x0 = np.random.randn(d)  # Initial iterate\n",
    "alpha = 0.5  # Step size reduction factor for line search\n",
    "beta = 0.8  # Sufficient decrease factor for line search\n",
    "\n",
    "# Solve the quadratic programming problem with Newton sketch method\n",
    "opt_gaps_newton5 = newton_sketch(A, c, m, x0, alpha, beta)\n",
    "\n",
    "# Print the optimality gaps for each iteration\n",
    "print(\"Optimality gaps for sketching Newton's Method with fixed sketch dimension:\")\n",
    "print(opt_gaps_newton5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39457263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quadratic programming problem\n",
    "n = 10  # Number of constraints\n",
    "d = 4 # Dimension of variables\n",
    "A = np.random.randn(n, d)  # Constraint matrix\n",
    "c = np.random.randn(d)  # Linear term\n",
    "m = 4*d  # Sketch dimension\n",
    "x0 = np.random.randn(d)  # Initial iterate\n",
    "alpha = 0.5  # Step size reduction factor for line search\n",
    "beta = 0.8  # Sufficient decrease factor for line search\n",
    "\n",
    "# Solve the quadratic programming problem with Newton sketch method\n",
    "opt_gaps_newton10 = newton_sketch(A, c, m, x0, alpha, beta)\n",
    "\n",
    "# Print the optimality gaps for each iteration\n",
    "print(\"Optimality gaps for sketching Newton's Method with fixed sketch dimension:\")\n",
    "print(opt_gaps_newton10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad96cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quadratic programming problem\n",
    "n = 15  # Number of constraints\n",
    "d = 4 # Dimension of variables\n",
    "A = np.random.randn(n, d)  # Constraint matrix\n",
    "c = np.random.randn(d)  # Linear term\n",
    "m = 6*d  # Sketch dimension\n",
    "x0 = np.random.randn(d)  # Initial iterate\n",
    "alpha = 0.5  # Step size reduction factor for line search\n",
    "beta = 0.8  # Sufficient decrease factor for line search\n",
    "\n",
    "# Solve the quadratic programming problem with Newton sketch method\n",
    "opt_gaps_newton15 = newton_sketch(A, c, m, x0, alpha, beta)\n",
    "\n",
    "# Print the optimality gaps for each iteration\n",
    "print(\"Optimality gaps for sketching Newton's Method with fixed sketch dimension:\")\n",
    "print(opt_gaps_newton15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quadratic programming problem\n",
    "n = 20  # Number of constraints\n",
    "d = 4 # Dimension of variables\n",
    "A = np.random.randn(n, d)  # Constraint matrix\n",
    "c = np.random.randn(d)  # Linear term\n",
    "m = 6*d  # Sketch dimension\n",
    "x0 = np.random.randn(d)  # Initial iterate\n",
    "alpha = 0.5  # Step size reduction factor for line search\n",
    "beta = 0.8  # Sufficient decrease factor for line search\n",
    "\n",
    "# Solve the quadratic programming problem with Newton sketch method\n",
    "opt_gaps_newton20 = newton_sketch(A, c, m, x0, alpha, beta)\n",
    "\n",
    "# Print the optimality gaps for each iteration\n",
    "print(\"Optimality gaps for sketching Newton's Method with fixed sketch dimension:\")\n",
    "print(opt_gaps_newton20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot opt_gaps by iteration\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(range(len(opt_gaps_newton5)), opt_gaps_newton5, label = 'Sketch Newton(n=5)')\n",
    "plt.plot(range(len(opt_gaps_newton10)), opt_gaps_newton10, label = 'Sketch Newton(n=10)')\n",
    "plt.plot(range(len(opt_gaps_newton15)), opt_gaps_newton15, label = 'Sketch Newton(n=15)')\n",
    "plt.plot(range(len(opt_gaps_newton20)), opt_gaps_newton20, label = 'Sketch Newton(n=20)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log Optimality Gap')\n",
    "plt.title('Log Optimality Gaps by Iteration for Newton Sketch')\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
